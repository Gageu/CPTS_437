{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_UrpF-YnyQd"
      },
      "source": [
        "**HOMEWORK 4**\n",
        "\n",
        "Assigned: October 19 (12:00PM noon)\n",
        "\n",
        "Due: November 09 (11:59PM midnight)\n",
        "\n",
        "This assignment consists five questions. One of them requires you to generate some Python code.\n",
        "\n",
        "--\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "(1) clone the notebook to your own Google Drive;\n",
        "\n",
        "(2) enter your answer and code to the cloned notebook directly; and\n",
        "\n",
        "(3) upload the shareable link of your notebook to Canvas (as your homework submission).\n",
        "\n",
        "--\n",
        "\n",
        "**Please do NOT attempt to enter your answer directly on the original homework notebook, which is only viewable. Late submission is only possible within two days, and a deduction of 3% per day will be applied.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKMV9Dj4ovbI"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmFEYDxbpn-I"
      },
      "source": [
        "**QUESTION 1** (20pts)\n",
        "\n",
        "Suppose you have a two-layer neural network with 3 hidden neurons, and 1 output neuron. Each hidden neuron accepts 3-dimensional input, is tanh-activated with the following form:\n",
        "\n",
        "$h_i(\\mathbf{x}) = \\mathrm{tanh}(\\mathbf{w}_i^\\top\\mathbf{x})$ where $1 \\leq i \\leq 3$.\n",
        "\n",
        "Next, the output neuron is $o(\\mathbf{z}) = \\boldsymbol{\\nu}^\\top\\mathbf{z}$ where $\\mathbf{z} = [z_1, z_2, z_3]$ with $z_i$ being the output of the corresponding hidden neuron $h_i(\\mathbf{x})$\n",
        "\n",
        "Let us initialize $\\mathbf{w}_i = [0.1, 0.1, 0.1]^\\top$ for all hidden neuron. Let's also initialize $\\boldsymbol{\\nu} = [0.0, 0.1, 0.2]^\\top$. Note that these notations suggest that $\\mathbf{w}_i$ and $\\boldsymbol{\\nu}$ are treated as column vectors, i.e., $[0.1, 0.1, 0.1]$ is a row vector so $[0.1, 0.1, 0.1]^\\top$ is a column vector.\n",
        "\n",
        "(1) Calculate the output of the network for the input $\\mathbf{x} = [0, 1, 1]^\\top$. (10pts)\n",
        "\n",
        "(2) Update all of the weights using backpropagation given that the desired/target output is 2.0. Use a learning rate of 0.5. (10pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the input vector and initial weights\n",
        "x = np.array([0, 1, 1])\n",
        "w = np.array([0.1, 0.1, 0.1])\n",
        "nu = np.array([0.0, 0.1, 0.2])\n",
        "\n",
        "# Calculate the output of each hidden neuron\n",
        "h = np.tanh(np.dot(w.T, x))\n",
        "\n",
        "# Since all hidden neurons are identical, their outputs are the same\n",
        "z = np.array([h, h, h])\n",
        "\n",
        "# Calculate the output of the network\n",
        "o = np.dot(nu.T, z)\n",
        "print(o)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phGCYQgm9v9j"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Nbe1K90mza"
      },
      "source": [
        "**QUESTION 2** (20pts)\n",
        "\n",
        "(1) Consider a variant of linear regression in which each data point $(\\mathbf{x}_i, y_i)$ is associated with a weighting factor $r_i > 0$. The sum-of-square error for a linear function $\\theta(\\mathbf{x}) = \\mathbf{w}^\\top\\mathbf{x}$ parameterized by $\\mathbf{w}$ in this case will be:\n",
        "\n",
        "$\\mathbf{E}(\\mathbf{w}) = \\sum_{i=1}^n r_i\\Big(\\mathbf{w}^\\top\\mathbf{x}_i - y_i\\Big)^2$ where $n$ is the number of data points.\n",
        "\n",
        "(a) Derive the gradient descent rule for this, i.e., derive $\\mathrm{d}\\mathbf{E}/\\mathrm{d}\\mathbf{w}$ (5pts).  \n",
        "\n",
        "(b) Derive the closed-form solution for the optimal $\\mathbf{w}$ that minimizes the above error. Show your work (5pts).\n",
        "\n",
        "(2) Consider a maximum likelihood estimation (MLE) problem with the following probabilistic model:\n",
        "\n",
        "$y_i = \\mathbf{w}^\\top \\mathbf{x}_i + \\epsilon_i$ where $\\epsilon_i \\sim \\mathbb{N}\\left(0, \\frac{1}{r_i}\\right)$ where $1 \\leq i \\leq n$.\n",
        "\n",
        "Show that this MLE model is equivalent to the above linear regression model with weighting factors. You can do that by following the below steps.\n",
        "\n",
        "(a) Find an analytically computable form for the likelihood of data given $\\mathbf{w}$, $\\mathbf{L}(\\mathbf{w}) \\triangleq P(y_1, y_2, \\ldots, y_n \\mid \\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n, \\mathbf{w})$. Show your work (5pts).\n",
        "\n",
        "(b) Find $\\mathbf{E}(\\mathbf{w}) = -log\\ \\mathbf{L}(\\mathbf{w})$ and the optimal $\\mathbf{w}$ (in closed-form) that minimizes $\\mathbf{E}(\\mathbf{w})$. Show your work. You shoud see that the closed-form solution for $\\mathbf{w}$ here is the same as that of (1) above (5pts)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLgLMO-L09cx"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sP_oAJ3F0_FD"
      },
      "source": [
        "**QUESTION 3** (10pts)\n",
        "\n",
        "Let $k(\\mathbf{x}, \\mathbf{x}') = (1 + \\mathbf{x}^\\top\\mathbf{x}')^2$ where $\\mathbf{x} = [x_1, x_2]^\\top$ and $\\mathbf{x}' = [x_1', x_2']$. Show that this is a valid kernel by explicitly constructing the feature vetor $\\phi(\\mathbf{x})$ such that regardless of the specific choice of $\\mathbf{x}$ and $\\mathbf{x}'$ (in $2$-dimensional space), we always have $k(\\mathbf{x}, \\mathbf{x}') = \\phi(\\mathbf{x})^\\top\\phi(\\mathbf{x}')$. You must be able to write each component of the vector $\\phi(\\mathbf{x})$ in terms of $x_1$ and $x_2$ only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4KaeWSuMsT5"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN8oAaxQNIcD"
      },
      "source": [
        "**QUESTION 4** (10pts)\n",
        "\n",
        "Consider a variant of the neural network in Question 1 in which we replace $h_i(\\mathbf{x}) = \\mathrm{tanh}(\\mathbf{w}_i^\\top\\mathbf{x})$ with $h_i(\\mathbf{x}) = 2\\mathrm{sigmoid}(2\\mathbf{w}_i^\\top\\mathbf{x}) - 1$. Show that on the same input and weight initialization, the outputs of this variant and the original neural network in Question 1 are always the same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVWg-10lSsfJ"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3wyxD1QSvfV"
      },
      "source": [
        "**QUESTION 5** (40pts)\n",
        "\n",
        "In this programming assignment, you will generate a learning curve for a linear regression algorithm applied on a diabete dataset.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html\n",
        "\n",
        "The x-axis for a learning curve is the number of training examples and the y-axis represents predictive performance. Your learning curve should contain two lines. The first plots performance (in this case, mean squared error) on the training data -- training curve. The second plots performance on a holdout test set (separate from the training data) -- test curve.\n",
        "\n",
        "(1) I would like you to implement the linear regression algorithm using the gradient descent method (instead of implementing its closed-form solution). Set learning rate = $0.001$ and number of update iterations = $100$.\n",
        "\n",
        "The x-axis range from 10\\% of the available data to 90\\%, in increments of 10 (which indicates the percentage of data used for training). The error should be calculated on a holdout set which contains 10\\% of the available data. You can do so by first partitioning the data into 10 equal folds: reserve one fold as test data, and iteratively use the 1, 2, ..., 9 of the remaining folds as training data. To ensure that your curve is representative of the true performance, repeat the above process 10 times and plot the average results. (20pts)\n",
        "\n",
        "(2) Next, build a second learning curve using the sklearn model linear_model.Ridge.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
        "\n",
        "This is a linear regression with a norm-2 regularization term that imposes a penalty for large coefficients, which we had discussed in class. (15pts)\n",
        "\n",
        "(3) Finally, offer some observations about the learning curves. What happens to the training curves and the test curves for both models as the number of training examples increases? What do these curves indicate about the bias of the model? (5pts)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
